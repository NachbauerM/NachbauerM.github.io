# Interesting: Goodhart's Law

According to Wikipedia: https://en.wikipedia.org/wiki/Goodhart%27s_law

"When a measure becomes a target, it ceases to be a good measure" and “Any observed statistical regularity will tend to collapse once pressure is placed upon it for control purposes.”

The Book “A very short introduction to Statistics”, by David J. Hand, provides an example why this can have real live impacts:
“When a particular measure is use as an indicator of the performance of a system, people may choose to target that measure, improving its value but at the cost of other aspects of the system. The chosen measure then improves disproportionately, and becomes useless as a measure of performance of the system. For example, the police could reduce the rate of shoplifting by focusing all their resources on it, at the cost of allowing other kinds of crime to rise. As a result, the rate of shoplifting becomes useless as an indicator of crime.”

Another nice example is the so called “Cobra Effect” (perverse incentive):
An Indian king who hates cobras promises a bounty for each cobra killed. Although he receives many cobras killed and distributes the bounties, the number of cobras does not decrease. After an investigation, the king finds that people have been breeding cobras to earn the bounty. He stops the bounty program. The cobra breeders no longer have an incentive to keep and release the cobras. This leads to an actual increase in the overall cobra population.

## My own thoughts on that
This law is very relevant in my current work. We work with lots of indicators that are supposed to measure our impacts and outputs. Very often those become individual items in our operational planning (being translated into activities) and the main focus of the project implementation, monitoring and reporting.
